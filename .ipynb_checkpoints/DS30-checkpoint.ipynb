{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Practise\n",
    "1. Python strikes a good balance between flexibility (it's interpreted) and speed (core machine-learning and linear algebra is in c via [cython](http://cython.org/)).\n",
    "2. Python strikes a good balance between being a programming language (it has a lot of language support built up by the community) and specialized machine-learning routines (e.g. [numpy](http://www.numpy.org/), [scipy](http://www.scipy.org/), [scikit learn](http://scikit-learn.org/), [pandas](http://pandas.pydata.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing graphics related libraries\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns  # plots are prettier with Seaborn\n",
    "import onlineldavb\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Image\n",
    "from IPython import display\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "\n",
    "# importing useful libraries\n",
    "import simplejson  # more efficient than the default json library\n",
    "import sys\n",
    "import requests  # better than the urllib libraries\n",
    "from requests_oauthlib import OAuth1\n",
    "from collections import Counter\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from itertools import islice, chain\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup twitter authorization\n",
    "\n",
    "1. Need to authenticate (can you guess why?).  Creat a twitter account and get your API credentials on the [Twitter API page](http://apps.twitter.com/).\n",
    "1. Secrets not checked into source control (good practice)!\n",
    "1. Using python `stream` (like an infinite list!)\n",
    "1. Handle stream using python `generator`\n",
    "1. *Protip*: `simplejson` is faster than builtin `json`.\n",
    "1. *Protip*: `requests` is easier to use than builtin `urllib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"twitter_secrets.json.nogit\") as fh:\n",
    "    secrets = simplejson.loads(fh.read())\n",
    "\n",
    "auth = OAuth1(\n",
    "    secrets[\"api_key\"],\n",
    "    secrets[\"api_secret\"],\n",
    "    secrets[\"access_token\"],\n",
    "    secrets[\"access_token_secret\"]\n",
    ")\n",
    "\n",
    "US_BOUNDING_BOX = \"-125.00,24.94,-66.93,49.59\"\n",
    "def tweet_generator():\n",
    "    \"\"\" Generator that live streams tweets (see 'yield' keyword)\"\"\"\n",
    "    stream = requests.post('https://stream.twitter.com/1.1/statuses/filter.json',\n",
    "                         auth=auth,\n",
    "                         stream=True,\n",
    "                         data={\"locations\" : US_BOUNDING_BOX})\n",
    "    \n",
    "    for line in stream.iter_lines():\n",
    "        if not line:  # filter out keep-alive new lines\n",
    "            continue\n",
    "        tweet = simplejson.loads(line)\n",
    "        if 'text' in tweet:\n",
    "            yield tweet['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out twitter stream\n",
    "\n",
    "1. When handling streams, need to use `itertools` (`slice` -> `islice`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in islice(tweet_generator(), 100):\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DISPLAY_EVERY = 20\n",
    "\n",
    "stop = set(stopwords.words('english'))  # predefined list of \"uninteresting\" words\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "def nlargest(n, word_scores):\n",
    "    \"\"\" Wrapper around heapq to return the n words with the largest count.\"\"\"\n",
    "    # word_scores: index 0=>word, 1=>count\n",
    "    return heapq.nlargest(n, word_scores, key=lambda x: x[1])\n",
    "\n",
    "try:\n",
    "    # islice allows you to get some # of values out of a generator.\n",
    "    for k, tweet in enumerate(islice(tweet_generator(), 1000)):\n",
    "        for word in tweet.lower().split():  # lowercase, split by whitespace\n",
    "            if word not in stop:  # avoid uninteresting words\n",
    "                counter[word] += 1\n",
    "        if k % DISPLAY_EVERY == (DISPLAY_EVERY - 1):  # only update text every so often\n",
    "            # \\r will overwrite updates, rather than listing them out\n",
    "            # one on each newline\n",
    "            sys.stdout.write(\"\\r\" + str(nlargest(10, counter.items())))\n",
    "except KeyboardInterrupt:\n",
    "    pass  # allow ctrl-c to exit the loop\n",
    "finally:\n",
    "    # Demo to show that Pandas has bar graphs...\n",
    "    # ...and that seaborn makes it pretty!\n",
    "    df = pd.DataFrame(nlargest(10, counter.items()), columns=['words', 'count'])\n",
    "    df.set_index('words').plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "1. Lists of word counts not as useful\n",
    "1. Wordcloud: size of word ~ frequency (there's a library for that)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "try:\n",
    "    for k, tweet in enumerate(islice(tweet_generator(), 1000)):\n",
    "        for word in tweet.lower().split():\n",
    "            if word not in stop and 'http' not in word:\n",
    "                counter[word] += 1\n",
    "        if k % DISPLAY_EVERY == (DISPLAY_EVERY - 1):\n",
    "            wordcloud = WordCloud().fit_words(counter.items())\n",
    "            plt.axis(\"off\")\n",
    "            display.clear_output(wait=True)\n",
    "            plt.imshow(wordcloud)\n",
    "            display.display(plt.gcf())\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "CLUSTER_SIZE = 4\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=CLUSTER_SIZE)\n",
    "\n",
    "def batch(iterable, size):\n",
    "    \"\"\" batch(\"ABCDEFG\", 3) -> ABC DEF G \"\"\"\n",
    "    sourceiter = iter(iterable)\n",
    "    while True:\n",
    "        batchiter = islice(sourceiter, size)\n",
    "        yield chain([batchiter.next()], batchiter)\n",
    "\n",
    "with open(\"dictnostops.txt\") as fh:\n",
    "    words = [line.strip() for line in fh.readlines()]\n",
    "    word_to_index = { word: k for k, word in enumerate(words) }\n",
    "\n",
    "def wordclouds(wordcounts):\n",
    "    wordclouds = [WordCloud().fit_words(counts) for counts in wordcounts]\n",
    "    fig, axes = plt.subplots(2,2)\n",
    "    display.clear_output(wait=True)\n",
    "    for k, (ax, wordcloud) in enumerate(zip(axes.flatten(), wordclouds)):\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.set_title(\"Topic %d\" % k)\n",
    "    display.display(fig)\n",
    "    fig.clear()\n",
    "\n",
    "try:\n",
    "    for tweets in batch(islice(tweet_generator(), 1000), BATCH_SIZE):\n",
    "        mat = sp.sparse.dok_matrix((BATCH_SIZE, len(words)))\n",
    "        for row, tweet in enumerate(tweets):\n",
    "            for word in tweet.lower().split():\n",
    "                if word in word_to_index:\n",
    "                    mat[row, word_to_index[word]] = 1.\n",
    "        kmeans.partial_fit(mat.tocsr())\n",
    "        wordcounts = [\n",
    "            nlargest(50, zip(words, kmeans.cluster_centers_[i]))\n",
    "            for i in xrange(kmeans.n_clusters)\n",
    "        ]\n",
    "        wordclouds(wordcounts)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "D = 1e9\n",
    "BATCH_SIZE = 20\n",
    "olda = onlineldavb.OnlineLDA(words, K, D, 1./K, 1./K, 1024., 0.7)\n",
    "\n",
    "try:\n",
    "    for tweets in batch(islice(tweet_generator(), 1000), BATCH_SIZE):\n",
    "        olda.update_lambda(list(tweets))\n",
    "        wordclouds(olda.topic_words(50))\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
